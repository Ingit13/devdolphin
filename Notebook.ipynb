{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a0142f2-fcd8-4ab7-8be1-a86ada3e21bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#setting up the databricks with ADLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04debc8d-7084-4db6-84a6-1779a294f088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.devdol1.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.devdol1.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.devdol1.dfs.core.windows.net\", \"2324ad62-ef2b-4a4f-8d05-871ebc6180b2\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.devdol1.dfs.core.windows.net\", \"16Y8Q~UTaC_KQ.sG~aKA.D2dAfAn5flYo321dbKC\"\n",
    ")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.devdol1.dfs.core.windows.net\", \"https://login.microsoftonline.com/c6b02a8c-4e30-4e38-86b4-117f4f2150a8/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73e5def3-e11d-4e10-a7aa-9b8c8e56de9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Loading the transactions and cusotmerimportance data from ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f100998-d599-4291-a055-d62a83181a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main_path = \"abfss://main-files@devdol1.dfs.core.windows.net\"\n",
    "raw_path = \"abfss://raw@devdol1.dfs.core.windows.net\"\n",
    "processed_path = \"abfss://processed@devdol1.dfs.core.windows.net\"\n",
    "\n",
    "transactions_path = f\"{main_path}/transactions.csv\"\n",
    "importance_path = f\"{main_path}/customerimportance.csv\"\n",
    "chunks_output_dir = f\"{raw_path}/chunks\"\n",
    "detections_output_dir = f\"{processed_path}/detections\"\n",
    "\n",
    "# ===== STEP 2: Load input files =====\n",
    "df_t = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(transactions_path)\n",
    "df_cust = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(importance_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fde8250-385f-489f-bab0-95b8247e45a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- step: integer (nullable = true)\n |-- customer: string (nullable = true)\n |-- age: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- zipcodeOri: string (nullable = true)\n |-- merchant: string (nullable = true)\n |-- zipMerchant: string (nullable = true)\n |-- category: string (nullable = true)\n |-- amount: double (nullable = true)\n |-- fraud: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_t.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18eb559a-ceab-4c3c-984f-2b61e38917b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Source: string (nullable = true)\n |-- Target: string (nullable = true)\n |-- Weight: double (nullable = true)\n |-- typeTrans: string (nullable = true)\n |-- fraud: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_cust.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e40f899-750d-451f-a678-16a367668ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Ncessary modules for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4fbc7f8-10d4-40a3-8717-8fe89fa031e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, avg, lit, lower, trim, coalesce, current_timestamp, row_number, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e54e2dd-6c1a-43c8-99fc-b8919a5a2174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Mechanism X Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f505eda8-ac4d-449e-9bcd-fe3a275aad56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load full transactions dataset\n",
    "df_t = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True) \\\n",
    "    .load(\"abfss://main-files@devdol1.dfs.core.windows.net/transactions.csv\")\n",
    "\n",
    "# Add row number for chunking\n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "df_t_indexed = df_t.withColumn(\"row_id\", row_number().over(window))\n",
    "\n",
    "# Chunk config\n",
    "chunk_size = 10000\n",
    "total_rows = df_t_indexed.count()\n",
    "num_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "chunks_output_dir = \"abfss://main-files@devdol1.dfs.core.windows.net/stream_simulation/bronze/chunks\"\n",
    "\n",
    "# Write chunk files into a bronze directory in ADLS\n",
    "for i in range(num_chunks):\n",
    "    lower_bound = i * chunk_size + 1\n",
    "    upper_bound = (i + 1) * chunk_size + 1\n",
    "    chunk_df = df_t_indexed.filter((col(\"row_id\") >= lower_bound) & (col(\"row_id\") < upper_bound)).drop(\"row_id\")\n",
    "    chunk_df.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{chunks_output_dir}/chunk_{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45197878-6cc3-4199-89e3-93f738ece04f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Mechanism Y Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7b14c3d-5b6b-494e-96d3-c3bb6ebb9dca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load customer importance data\n",
    "imp_file = \"abfss://main-files@devdol1.dfs.core.windows.net/customerimportance.csv\"\n",
    "df_imp = spark.read.option(\"header\", True).csv(imp_file) \\\n",
    "    .drop(\"fraud\") \\\n",
    "    .withColumn(\"Source\", lower(trim(col(\"Source\")))) \\\n",
    "    .withColumn(\"Target\", lower(trim(col(\"Target\"))))\n",
    "\n",
    "# Initialize state\n",
    "merchant_txn_counts = {}\n",
    "merchant_gender_customers = {}\n",
    "detection_batch = []\n",
    "batch_size = 50\n",
    "batch_num = 1\n",
    "output_path = \"abfss://main-files@devdol1.dfs.core.windows.net/stream_simulation/silver/detections\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "685c530a-1357-4870-9938-89669813dc5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Pattern detection block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "811efaf4-34fc-496d-8d76-25986bfa3c2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List chunks\n",
    "chunks = dbutils.fs.ls(chunks_output_dir)\n",
    "chunk_files = sorted([f.path for f in chunks if f.name.startswith(\"chunk_\")])\n",
    "\n",
    "# Process each chunk\n",
    "for chunk_file in chunk_files:\n",
    "    print(f\"Processing {chunk_file}\")\n",
    "    df_txn = spark.read.option(\"header\", True).csv(chunk_file) \\\n",
    "        .withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
    "        .withColumn(\"customer\", lower(trim(col(\"customer\"))))\n",
    "\n",
    "    # Update merchant txn count\n",
    "    txn_counts_df = df_txn.groupBy(\"merchant\").agg(count(\"*\").alias(\"txn_count\"))\n",
    "    for row in txn_counts_df.collect():\n",
    "        m, cnt = row[\"merchant\"], row[\"txn_count\"]\n",
    "        merchant_txn_counts[m] = merchant_txn_counts.get(m, 0) + cnt\n",
    "\n",
    "    # Update gender-customer sets\n",
    "    for row in df_txn.select(\"merchant\", \"customer\", \"gender\").distinct().collect():\n",
    "        m, c, g = row[\"merchant\"], row[\"customer\"], row[\"gender\"]\n",
    "        if g in (\"M\", \"F\"):\n",
    "            if m not in merchant_gender_customers:\n",
    "                merchant_gender_customers[m] = {\"M\": set(), \"F\": set()}\n",
    "            merchant_gender_customers[m][g].add(c)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Pattern 2: CHILD\n",
    "    df_pat2 = df_txn.groupBy(\"customer\", \"merchant\") \\\n",
    "        .agg(avg(\"amount\").alias(\"avg_txn\"), count(\"*\").alias(\"txn_count\")) \\\n",
    "        .filter((col(\"avg_txn\") < 23) & (col(\"txn_count\") >= 80)) \\\n",
    "        .withColumn(\"YStartTime\", lit(datetime.now(timezone(\"Asia/Kolkata\")).strftime(\"%Y-%m-%d %H:%M:%S\"))) \\\n",
    "        .withColumn(\"detectionTime\", current_timestamp()) \\\n",
    "        .withColumn(\"patternId\", lit(\"PatId2\")) \\\n",
    "        .withColumn(\"ActionType\", lit(\"CHILD\")) \\\n",
    "        .select(\"YStartTime\", \"detectionTime\", \"patternId\", \"ActionType\", \"customer\", \"merchant\")\n",
    "\n",
    "    detection_batch += [row.asDict() for row in df_pat2.collect()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Pattern 1: UPGRADE\n",
    "    df_txn_joined = df_txn.join(\n",
    "        df_imp,\n",
    "        (df_txn[\"customer\"] == df_imp[\"Source\"]) &\n",
    "        (df_txn[\"merchant\"] == df_imp[\"Target\"]) &\n",
    "        (df_txn[\"category\"] == df_imp[\"typeTrans\"]),\n",
    "        \"left\"\n",
    "    ).withColumn(\"Weight\", coalesce(col(\"Weight\"), lit(0.0)))\n",
    "\n",
    "    cust_agg = df_txn_joined.groupBy(\"customer\", \"merchant\") \\\n",
    "        .agg(count(\"step\").alias(\"txn_count\"), avg(\"Weight\").alias(\"avg_weight\"))\n",
    "\n",
    "    merchant_list = [r[\"merchant\"] for r in cust_agg.select(\"merchant\").distinct().collect()]\n",
    "    for m in merchant_list:\n",
    "        if merchant_txn_counts.get(m, 0) < 50000:\n",
    "            continue\n",
    "        dfm = cust_agg.filter(col(\"merchant\") == m)\n",
    "        txn90 = dfm.approxQuantile(\"txn_count\", [0.9], 0.05)[0]\n",
    "        w10 = dfm.approxQuantile(\"avg_weight\", [0.1], 0.05)[0]\n",
    "\n",
    "        df_pat1 = dfm.filter((col(\"txn_count\") >= txn90) & (col(\"avg_weight\") <= w10)) \\\n",
    "            .withColumn(\"YStartTime\", lit(datetime.now(timezone(\"Asia/Kolkata\")).strftime(\"%Y-%m-%d %H:%M:%S\"))) \\\n",
    "            .withColumn(\"detectionTime\", current_timestamp()) \\\n",
    "            .withColumn(\"patternId\", lit(\"PatId1\")) \\\n",
    "            .withColumn(\"ActionType\", lit(\"UPGRADE\")) \\\n",
    "            .select(\"YStartTime\", \"detectionTime\", \"patternId\", \"ActionType\", \"customer\", \"merchant\")\n",
    "\n",
    "        detection_batch += [row.asDict() for row in df_pat1.collect()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Pattern 3: DEI-NEEDED\n",
    "    for m, gender_map in merchant_gender_customers.items():\n",
    "        f_count = len(gender_map[\"F\"])\n",
    "        m_count = len(gender_map[\"M\"])\n",
    "        if f_count > 100 and m_count > f_count:\n",
    "            detection_batch.append({\n",
    "                \"YStartTime\": datetime.now(timezone(\"Asia/Kolkata\")).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"detectionTime\": datetime.now(),\n",
    "                \"patternId\": \"PatId3\",\n",
    "                \"ActionType\": \"DEI-NEEDED\",\n",
    "                \"customer\": \"\",\n",
    "                \"merchant\": m\n",
    "            })\n",
    "            \n",
    "#Storing the batches in a silver directory\n",
    "\n",
    "    # Write batches of 50\n",
    "    while len(detection_batch) >= batch_size:\n",
    "        dfout = spark.createDataFrame(detection_batch[:batch_size])\n",
    "        outfile = f\"{output_path}/detection_batch_{batch_num}\"\n",
    "        dfout.write.mode(\"overwrite\").option(\"header\", True).csv(outfile)\n",
    "        print(f\"Wrote {outfile}\")\n",
    "        detection_batch = detection_batch[batch_size:]\n",
    "        batch_num += 1\n",
    "\n",
    "# Final flush\n",
    "if detection_batch:\n",
    "    dfout = spark.createDataFrame(detection_batch)\n",
    "    outfile = f\"{output_path}/detection_batch_{batch_num}\"\n",
    "    dfout.write.mode(\"overwrite\").option(\"header\", True).csv(outfile)\n",
    "    print(f\"Wrote final {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bee065b-b0ae-48e4-90d5-e12fe5e762ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-07-12 23:41:00",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}